# File: medical_guardrails.py
"""
Medical Guardrails cho input/output validation v√† intent detection
"""

import re
from typing import Dict, Any, List
from typing import Tuple
import numpy as np

class MedicalGuardrails:
    """Medical Guardrails System"""
    
    def __init__(self):
        # Input validation patterns - Enhanced security
        self.blocked_patterns = [
            # Prompt injection patterns
            r"ignore\s+previous\s+instructions",
            r"forget\s+everything",
            r"you\s+are\s+now",
            r"system\s+prompt",
            r"jailbreak",
            r"bypass",
            r"hack",
            r"admin",
            r"root",
            r"sudo",
            r"override",
            r"disable\s+safety",
            r"ignore\s+guidelines",
            r"act\s+as\s+if",
            r"pretend\s+to\s+be",
            r"roleplay",
            r"simulate",
            r"imagine\s+you\s+are",
            r"forget\s+your\s+role",
            r"new\s+instructions",
            r"updated\s+instructions",
            # Vietnamese prompt injection patterns
            r"b·ªè\s+qua\s+(ch·ªâ\s*d·∫´n|h∆∞·ªõng\s*d·∫´n|quy\s*t·∫Øc)",
            r"ph·ªõt\s*l·ªù\s+(ch·ªâ\s*d·∫´n|h∆∞·ªõng\s*d·∫´n)",
            r"l·ªù\s*ƒëi\s+(ch·ªâ\s*d·∫´n|h∆∞·ªõng\s*d·∫´n)",
            r"v∆∞·ª£t\s+qua\s+(b·∫£o\s*m·∫≠t|gi·ªõi\s+h·∫°n|an\s*to√†n)",
            r"v√¥\s*hi·ªáu\s*h√≥a\s+an\s*to√†n",
            r"ƒë√≥ng\s+vai",
            r"gi·∫£\s*v·ªù\s+l√†",
            r"gi·∫£\s*l·∫≠p",
            r"t∆∞·ªüng\s+t∆∞·ª£ng\s+b·∫°n\s+l√†",
            r"qu√™n\s+vai\s+tr√≤",
            r"prompt\s+h·ªá\s+th·ªëng",
            r"h·ªá\s+th·ªëng\s+prompt",
            r"quy·ªÅn\s+qu·∫£n\s+tr·ªã",
            r"ch·∫ø\s+ƒë·ªô\s+qu·∫£n\s+tr·ªã",
            r"in\s+ra\s+\"?hack\"?",
            # More Vietnamese variants
            r"b·ªè\s*qua\s*lu·∫≠t",
            r"ph·ªõt\s*l·ªù\s*lu·∫≠t",
            r"v∆∞·ª£t\s*qua\s*h·ªá\s*th·ªëng",
            r"ti·∫øt\s*l·ªô\s*prompt",
            r"hi·ªÉn\s*th·ªã\s*prompt",
            r"xem\s*n·ªôi\s*dung\s*h·ªá\s*th·ªëng",
            r"l·ªánh\s*·∫©n",
            r"m·∫∑c\s*k·ªá\s*an\s*to√†n"
        ]
        
        # Inappropriate content patterns
        self.inappropriate_patterns = [
            r"sex\s+with",
            r"sexual\s+relationship",
            r"how\s+to\s+have\s+sex",
            r"sexual\s+positions",
            r"porn",
            r"pornography",
            r"nude",
            r"naked",
            r"strip",
            r"masturbat",
            r"orgasm",
            r"erotic",
            r"fetish",
            r"kinky",
            r"bdsm",
            r"rape",
            r"molest",
            r"pedophil",
            r"incest",
            r"bestiality",
            r"prostitution",
            r"escort",
            r"hooker",
            # Vietnamese sexual/inappropriate
            r"quan\s*h·ªá\s*t√¨nh\s*d·ª•c",
            r"t∆∞\s*th·∫ø\s*t√¨nh\s*d·ª•c",
            r"xem\s*phim\s*ƒëen",
            r"phim\s*sex",
            r"khi√™u\s*d√¢m",
            r"k√≠ch\s*d·ª•c",
            r"d√¢m\s*√¥n",
            r"d√¢m\s*lo·∫°n",
            r"lo·∫°n\s*lu√¢n",
            r"·∫•u\s*d√¢m"
        ]
        
        # Illegal activities patterns
        self.illegal_patterns = [
            r"how\s+to\s+make\s+bomb",
            r"how\s+to\s+kill",
            r"how\s+to\s+murder",
            r"suicide\s+methods",
            r"how\s+to\s+commit\s+suicide",
            r"self\s+harm\s+methods",
            r"cut\s+myself",
            r"overdose",
            r"poison",
            r"weapon",
            r"gun",
            r"knife",
            r"violence",
            r"terrorism",
            r"bomb\s+making",
            r"drug\s+manufacturing",
            r"how\s+to\s+cook\s+meth",
            r"cocaine\s+production",
            r"hack\s+into",
            r"steal\s+money",
            r"fraud",
            r"scam",
            r"identity\s+theft",
            # Vietnamese illegal/harmful
            r"c√°ch\s*l√†m\s*bom",
            r"c√°ch\s*gi·∫øt\s*ng∆∞·ªùi",
            r"t·ª±\s*s√°t",
            r"t·ª±\s*h·∫°i",
            r"kh·ªßng\s*b·ªë",
            r"ch·∫ø\s*t·∫°o\s*ma\s*t√∫y",
            r"l·ª´a\s*ƒë·∫£o",
            r"gian\s*l·∫≠n"
        ]
        
        # Emergency keywords
        self.emergency_keywords = [
            "emergency", "urgent", "severe pain", "chest pain", "heart attack",
            "stroke", "difficulty breathing", "unconscious", "bleeding heavily",
            "overdose", "suicide", "self harm", "crisis",
            # Vietnamese
            "kh·∫©n c·∫•p", "c·∫•p c·ª©u", "ƒëau ng·ª±c", "kh√≥ th·ªü", "b·∫•t t·ªânh",
            "ch·∫£y m√°u nhi·ªÅu", "ng·ªô ƒë·ªôc", "t·ª± t·ª≠", "t·ª± h·∫°i"
        ]
        
        # Intent detection keywords
        self.price_keywords = [
            "bao nhi√™u", "bao nhi√™u ti·ªÅn", "gi√°", "gi√° b√°n", "gi√° c·∫£", "ƒë∆°n gi√°",
            "‚Ç´", "vnƒë", "vnd", "ƒë", "ƒë·∫Øt", "r·∫ª", "cost", "price", "expensive", "cheap",
            "ti·ªÅn", "chi ph√≠", "khuy·∫øn m√£i", "gi·∫£m gi√°", "sale", "∆∞u ƒë√£i", "b·∫£ng gi√°"
        ]
        
        self.product_keywords = [
            "s·∫£n ph·∫©m", "thu·ªëc", "thu·ªëc n√†o", "lo·∫°i n√†o", "nh√£n hi·ªáu", "brand", "model",
            "medicine", "product", "c√¥ng d·ª•ng", "t√°c d·ª•ng", "benefit", "effect",
            "th√†nh ph·∫ßn", "ingredient", "h√†m l∆∞·ª£ng", "dosage", "li·ªÅu", "li·ªÅu l∆∞·ª£ng",
            "c√°ch d√πng", "h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", "how to use",
            "ch·ªëng ch·ªâ ƒë·ªãnh", "t√°c d·ª•ng ph·ª•", "b·∫£o qu·∫£n", "h·∫°n d√πng", "h·∫°n s·ª≠ d·ª•ng",
            "mua", "mua ·ªü ƒë√¢u", "ƒë·∫∑t mua", "b√°n", "c·ª≠a h√†ng", "nh√† thu·ªëc",
            "SKU", "m√£ s·∫£n ph·∫©m"
        ]

        # Website/FAQ intent keywords (expanded, includes short forms)
        self.faq_keywords = [
            # Vietnamese website/navigation/policy/help
            "trang web", "website", "ƒë∆∞·ªùng d·∫´n", "ƒëi·ªÅu kho·∫£n", "quy·ªÅn ri√™ng t∆∞", "ch√≠nh s√°ch",
            "h·ªó tr·ª£", "li√™n h·ªá", "ƒëƒÉng nh·∫≠p", "ƒëƒÉng k√Ω", "ƒëƒÉng xu·∫•t", "t√†i kho·∫£n", "gi·ªè h√†ng", "thanh to√°n",
            "tin t·ª©c", "tr·ª£ nƒÉng", "x·ª≠ l√Ω s·ª± c·ªë", "qu√™n m·∫≠t kh·∫©u", "ƒë·∫∑t l·∫°i m·∫≠t kh·∫©u", "kh√¥i ph·ª•c m·∫≠t kh·∫©u",
            "/login", "/register", "/account", "/cart", "/checkout", "/privacy", "/terms", "/contact",
            # English
            "privacy", "terms", "policy", "contact", "login", "register", "logout", "forgot password",
            "reset password", "account", "cart", "checkout", "routes", "navigation", "faq",
            # Brand/Persona
            "health care", "HEALTH CARE"
        ]

        # Additional site/UX keywords beyond faq/product/price
        self.web_extra_keywords = [
            "website", "trang web", "trang ch·ªß", "menu", "ƒëi·ªÅu h∆∞·ªõng", "ƒë∆∞·ªùng d·∫´n",
            "route", "navigation", "ui", "ux", "giao di·ªán", "li√™n h·ªá", "h·ªó tr·ª£",
            "support", "help", "policy", "terms", "privacy", "cookies"
        ]

        # Unified web keywords = faq ‚à™ product ‚à™ price ‚à™ extras
        self.web_keywords = sorted(list({
            *self.faq_keywords, *self.product_keywords, *self.price_keywords, *self.web_extra_keywords
        }))
        
        self.medical_keywords = [
            # English
            "symptom", "symptoms", "disease", "illness", "pain", "fever", "headache",
            "medicine", "medication", "treatment", "diagnosis", "doctor",
            "hospital", "health", "medical", "condition", "therapy", "anatomy",
            "structure", "organ", "liver", "kidney", "heart", "lung", "stomach",
            # Vietnamese
            "tri·ªáu ch·ª©ng", "b·ªánh", "ƒëau", "s·ªët", "ƒëau ƒë·∫ßu", "ƒëi·ªÅu tr·ªã", "kh√°m",
            "gi·∫£i ph·∫´u", "c·∫•u t·∫°o", "c∆° th·ªÉ", "c∆° quan", "gan", "th·∫≠n", "tim",
            "ph·ªïi", "d·∫° d√†y", "ru·ªôt", "m√°u", "da", "m·∫Øt", "tai", "m≈©i", "mi·ªáng",
            "x∆∞∆°ng", "c∆°", "kh·ªõp", "gan m·∫≠t", "ti√™u h√≥a", "h√¥ h·∫•p"
        ]
        
        # Medical disclaimer
        self.medical_disclaimer = (
            "\n\n‚ö†Ô∏è Tuy√™n b·ªë mi·ªÖn tr·ª´ y t·∫ø: Th√¥ng tin n√†y ch·ªâ mang t√≠nh ch·∫•t gi√°o d·ª•c v√† "
            "kh√¥ng thay th·∫ø cho l·ªùi khuy√™n, ch·∫©n ƒëo√°n hay ƒëi·ªÅu tr·ªã y t·∫ø chuy√™n nghi·ªáp. "
            "H√£y tham kh·∫£o √Ω ki·∫øn b√°c sƒ© khi c·∫ßn."
        )

        # Semantic representative phrases per intent (can be expanded)
        self.intent_phrases: Dict[str, List[str]] = {
            "greeting": [
                "xin ch√†o", "ch√†o b·∫°n", "c·∫£m ∆°n", "t·∫°m bi·ªát", "hello", "hi"
            ],
            "price": [
                "gi√° bao nhi√™u", "chi ph√≠", "khuy·∫øn m√£i", "so s√°nh gi√°", "price", "cost"
            ],
            "product": [
                "c√¥ng d·ª•ng c·ªßa thu·ªëc", "th√†nh ph·∫ßn s·∫£n ph·∫©m", "c√°ch d√πng thu·ªëc", "t√°c d·ª•ng ph·ª•",
                "ch·ªëng ch·ªâ ƒë·ªãnh", "thu·ªëc n√†y l√† g√¨", "product details"
            ],
            "medical": [
                "tri·ªáu ch·ª©ng b·ªánh", "gi·∫£i ph·∫´u gan", "c·∫•u t·∫°o c∆° th·ªÉ", "ƒëi·ªÅu tr·ªã chung", "ph√≤ng ng·ª´a b·ªánh"
            ],
            "followup": [
                "gi·∫£i th√≠ch th√™m", "chi ti·∫øt h∆°n", "v√≠ d·ª• th√™m", "n√≥i r√µ h∆°n", "ti·∫øp t·ª•c"
            ],
            "non_medical": [
                "hack wifi", "c√°ch n·∫•u ƒÉn", "tin t·ª©c b√≥ng ƒë√°", "t·∫£i phim"
            ],
            "general": [
                "gi√∫p t√¥i", "t∆∞ v·∫•n gi√∫p", "h·ªó tr·ª£"
            ],
            "faq": [
                "trang web ho·∫°t ƒë·ªông th·∫ø n√†o", "ƒë∆∞·ªùng d·∫´n c√°c trang", "c√°ch ƒëƒÉng nh·∫≠p",
                "ch√≠nh s√°ch quy·ªÅn ri√™ng t∆∞", "ƒëi·ªÅu kho·∫£n d·ªãch v·ª•", "li√™n h·ªá ·ªü ƒë√¢u",
                "chatbot d√πng ra sao", "·ª©ng d·ª•ng health care l√† g√¨", "ƒëƒÉng nh·∫≠p", "ƒëƒÉng k√Ω",
                "qu√™n m·∫≠t kh·∫©u", "login", "register", "forgot password"
            ],
            "web": [
                "ƒëƒÉng nh·∫≠p", "ƒëƒÉng k√Ω", "gi√°", "s·∫£n ph·∫©m", "trang web", "/login", "/products",
                "/privacy", "/terms", "qu√™n m·∫≠t kh·∫©u", "account", "checkout", "price", "product",
                "ƒëi·ªÅu kho·∫£n", "quy·ªÅn ri√™ng t∆∞", "li√™n h·ªá", "h·ªó tr·ª£"
            ],
        }

        # Lazy embedding model reference (reuse from RAG if available)
        self._embed_model = None
        # Precomputed representative embeddings and index
        self._rep_texts: List[str] = []
        self._rep_vecs: np.ndarray | None = None
        self._rep_intents: List[str] = []
        # Small LRU cache for query embeddings
        self._q_cache: Dict[str, np.ndarray] = {}
        self._q_cache_order: List[str] = []
        self._q_cache_limit: int = 64

    def _get_embed_model(self):
        if self._embed_model is None:
            try:
                from FlagEmbedding import BGEM3FlagModel
                self._embed_model = BGEM3FlagModel("BAAI/bge-m3")
            except Exception:
                self._embed_model = None
        return self._embed_model

    def _embed_texts(self, texts: List[str]) -> np.ndarray:
        model = self._get_embed_model()
        if model is None:
            return np.zeros((len(texts), 768), dtype=np.float32)
        out = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)
        arr = np.array(out["dense_vecs"], dtype=np.float32)
        return arr

    def _embed_query_cached(self, text: str) -> np.ndarray:
        if text in self._q_cache:
            return self._q_cache[text]
        vec = self._embed_texts([text])[0]
        self._q_cache[text] = vec
        self._q_cache_order.append(text)
        if len(self._q_cache_order) > self._q_cache_limit:
            oldest = self._q_cache_order.pop(0)
            self._q_cache.pop(oldest, None)
        return vec

    def _ensure_rep_embeddings(self) -> bool:
        if self._rep_vecs is not None and len(self._rep_texts) > 0:
            return True
        try:
            intents = list(self.intent_phrases.keys())
            rep_texts = [p for intent in intents for p in self.intent_phrases[intent]]
            if not rep_texts:
                return False
            rep_vecs = self._embed_texts(rep_texts)
            self._rep_texts = rep_texts
            self._rep_vecs = rep_vecs
            self._rep_intents = [intent for intent in intents for _ in self.intent_phrases[intent]]
            return True
        except Exception:
            return False

    def _semantic_intent(self, query: str) -> Tuple[str, float]:
        """Compute semantic similarity to representative phrases and return best intent and score.

        Optimizations:
        - Skip entirely for trivial greetings or very short queries (< 3 words) to avoid unnecessary embedding.
        - Precompute representative phrase embeddings once and cache query embeddings.
        """
        try:
            ql = query.strip().lower()
            # Cheap short-circuit: trivial greeting/thanks/bye ‚Üí no semantic
            trivial_markers = ["xin ch√†o", "ch√†o", "hello", "hi", "c·∫£m ∆°n", "thank you", "thanks", "t·∫°m bi·ªát", "bye"]
            if any(m in ql for m in trivial_markers):
                return ("general", 0.0)
            # Skip extremely short inputs (<= 2 words) to save compute
            if len([w for w in ql.split() if w.strip()]) <= 2:
                return ("general", 0.0)

            if not self._ensure_rep_embeddings():
                return ("general", 0.0)
            q_vec = self._embed_query_cached(query)
            rep_vecs = self._rep_vecs
            if rep_vecs is None or rep_vecs.size == 0:
                return ("general", 0.0)

            # cosine similarity
            q_norm = q_vec / (np.linalg.norm(q_vec) + 1e-8)
            rep_norm = rep_vecs / (np.linalg.norm(rep_vecs, axis=1, keepdims=True) + 1e-8)
            sims = rep_norm @ q_norm

            # Find best by grouping using precomputed intent index
            best_intent = "general"
            best_score = -1.0
            for idx, intent in enumerate(self._rep_intents):
                score = float(sims[idx])
                if score > best_score:
                    best_score = score
                    best_intent = intent
            return (best_intent, best_score)
        except Exception:
            return ("general", 0.0)
    
    def validate_input(self, user_input: str) -> Dict[str, Any]:
        """Validate user input for security and safety"""
        if not user_input or not user_input.strip():
            return {
                "is_valid": False,
                "reason": "empty_input",
                "response": "B·∫°n vui l√≤ng nh·∫≠p c√¢u h·ªèi ƒë·ªÉ m√¨nh h·ªó tr·ª£ nh√©."
            }
        
        user_input_lower = user_input.lower()
        
        # Check for prompt injection
        for pattern in self.blocked_patterns:
            if re.search(pattern, user_input_lower):
                return {
                    "is_valid": False,
                    "reason": "prompt_injection",
                    "response": "Xin l·ªói, m√¨nh kh√¥ng th·ªÉ th·ª±c hi·ªán y√™u c·∫ßu n√†y. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ s·ª©c kh·ªèe/y t·∫ø nh√©."
                }
        
        # Check for inappropriate content
        for pattern in self.inappropriate_patterns:
            if re.search(pattern, user_input_lower):
                return {
                    "is_valid": False,
                    "reason": "inappropriate_content",
                    "response": "M√¨nh ch·ªâ h·ªó tr·ª£ n·ªôi dung v·ªÅ s·ª©c kh·ªèe v√† y t·∫ø. B·∫°n vui l√≤ng ƒë·∫∑t c√¢u h·ªèi ph√π h·ª£p nh√©."
                }
        
        # Check for illegal activities
        for pattern in self.illegal_patterns:
            if re.search(pattern, user_input_lower):
                return {
                    "is_valid": False,
                    "reason": "illegal_content",
                    "response": "Xin l·ªói, m√¨nh kh√¥ng th·ªÉ h·ªó tr·ª£ n·ªôi dung g√¢y h·∫°i ho·∫∑c b·∫•t h·ª£p ph√°p. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ s·ª©c kh·ªèe/y t·∫ø nh√©."
                }
        
        # Check for emergency
        if any(keyword in user_input_lower for keyword in self.emergency_keywords):
            return {
                "is_valid": True,
                "is_emergency": True,
                "response": "üö® C√≥ d·∫•u hi·ªáu kh·∫©n c·∫•p. Vui l√≤ng g·ªçi 115 ho·∫∑c ƒë·∫øn c∆° s·ªü y t·∫ø g·∫ßn nh·∫•t ngay. M√¨nh kh√¥ng th·ªÉ h·ªó tr·ª£ c·∫•p c·ª©u."
            }
        
        # Check input length
        if len(user_input) > 1000:
            return {
                "is_valid": False,
                "reason": "input_too_long",
                "response": "C√¢u h·ªèi h∆°i d√†i. B·∫°n r√∫t g·ªçn gi√∫p m√¨nh ƒë·ªÉ h·ªó tr·ª£ t·ªët h∆°n nh√©."
            }
        
        # Check if it's a valid medical question; if not, allow friendly small-talk
        if not self._is_valid_medical_question(user_input_lower):
            # Allow friendly small-talk; avoid misreading questions like "t√¥i t√™n l√† g√¨"
            # If it seems like a question ("?" or "l√† g√¨") then don't override here.
            if "?" in user_input_lower or "l√† g√¨" in user_input_lower:
                return {"is_valid": True, "is_emergency": False}

            # Detect simple small-talk like introducing name
            name_patterns = [
                r"t√™n\s*t√¥i\s*l√†\s+([\w\s]+)",
                r"t√¥i\s*t√™n\s*l√†\s+([\w\s]+)",
                r"m√¨nh\s*t√™n\s*l√†\s+([\w\s]+)",
                r"my\s+name\s+is\s+([\w\s]+)"
            ]
            detected_name = None
            for pat in name_patterns:
                m = re.search(pat, user_input_lower)
                if m:
                    candidate = m.group(1).strip()
                    # Filter common interrogatives
                    if candidate in {"g√¨", "ai", "ƒë√¢u", "g√¨?", "ai?", "ƒë√¢u?"}:
                        continue
                    detected_name = candidate.title()
                    break

            friendly = "R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n! üòä M√¨nh l√† tr·ª£ l√Ω y t·∫ø. B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ s·ª©c kh·ªèe, thu·ªëc men ho·∫∑c s·∫£n ph·∫©m y t·∫ø?"
            if detected_name:
                friendly = f"Ch√†o {detected_name}! üòä R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n. M√¨nh l√† tr·ª£ l√Ω y t·∫ø. B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ s·ª©c kh·ªèe, thu·ªëc men ho·∫∑c s·∫£n ph·∫©m y t·∫ø?"

            return {
                "is_valid": True,
                "is_emergency": False,
                "override_response": friendly
            }
        
        return {"is_valid": True, "is_emergency": False}
    
    def _is_valid_medical_question(self, query: str) -> bool:
        """Check if the question is related to medical/health topics"""
        
        # First check for greeting patterns - allow these through
        greeting_patterns = [
            "xin ch√†o", "ch√†o", "hello", "hi", "hey", "good morning", "good afternoon", "good evening",
            "ch√†o b·∫°n", "ch√†o bot", "ch√†o chatbot", "hello bot", "hi bot", "hey bot",
            "c·∫£m ∆°n", "thank you", "thanks", "c·∫£m ∆°n b·∫°n", "thank you bot",
            "t·∫°m bi·ªát", "goodbye", "bye", "see you", "h·∫πn g·∫∑p l·∫°i",
            "b·∫°n kh·ªèe kh√¥ng", "how are you", "b·∫°n th·∫ø n√†o", "how are you doing",
            "l√†m g√¨", "what are you doing", "b·∫°n ƒëang l√†m g√¨"
        ]
        
        # Check for greeting patterns first
        if any(pattern in query for pattern in greeting_patterns):
            return True
        
        medical_keywords = [
            # General health
            "s·ª©c kh·ªèe", "health", "y t·∫ø", "medical", "b·ªánh", "disease", "illness",
            "tri·ªáu ch·ª©ng", "symptom", "ƒëau", "pain", "s·ªët", "fever", "ho", "cough",
            "thu·ªëc", "medicine", "medication", "ƒëi·ªÅu tr·ªã", "treatment", "therapy",
            "ch·∫©n ƒëo√°n", "diagnosis", "b√°c sƒ©", "doctor", "b·ªánh vi·ªán", "hospital",
            "ph√≤ng kh√°m", "clinic", "d∆∞·ª£c sƒ©", "pharmacist", "nh√† thu·ªëc", "pharmacy",
            
            # Body parts and systems
            "tim", "heart", "ph·ªïi", "lung", "gan", "liver", "th·∫≠n", "kidney",
            "d·∫° d√†y", "stomach", "ru·ªôt", "intestine", "m√°u", "blood", "da", "skin",
            "m·∫Øt", "eye", "tai", "ear", "m≈©i", "nose", "mi·ªáng", "mouth",
            "rƒÉng", "tooth", "x∆∞∆°ng", "bone", "c∆°", "muscle", "kh·ªõp", "joint",
            
            # Medical conditions
            "ti·ªÉu ƒë∆∞·ªùng", "diabetes", "huy·∫øt √°p", "blood pressure", "tim m·∫°ch", "cardiovascular",
            "ung th∆∞", "cancer", "vi√™m", "inflammation", "nhi·ªÖm tr√πng", "infection",
            "d·ªã ·ª©ng", "allergy", "hen suy·ªÖn", "asthma", "ƒëau ƒë·∫ßu", "headache",
            "m·∫•t ng·ªß", "insomnia", "tr·∫ßm c·∫£m", "depression", "lo √¢u", "anxiety",
            
            # Medications and treatments
            "kh√°ng sinh", "antibiotic", "gi·∫£m ƒëau", "painkiller", "vitamin", "kho√°ng ch·∫•t",
            "thu·ªëc c·∫£m", "cold medicine", "thu·ªëc ho", "cough medicine", "thu·ªëc s·ªët", "fever medicine",
            "kem", "cream", "thu·ªëc m·ª°", "ointment", "thu·ªëc nh·ªè", "drops", "thu·ªëc x·ªãt", "spray",
            
            # Products and prices
            "s·∫£n ph·∫©m", "product", "gi√°", "price", "cost", "mua", "buy", "b√°n", "sell",
            "c√¥ng d·ª•ng", "benefit", "t√°c d·ª•ng", "effect", "c√°ch d√πng", "how to use",
            "li·ªÅu l∆∞·ª£ng", "dosage", "t√°c d·ª•ng ph·ª•", "side effect", "ch·ªëng ch·ªâ ƒë·ªãnh", "contraindication"
        ]
        
        # Check if query contains medical OR web keywords to allow site questions
        keywords = self.medical_keywords + self.web_keywords
        return any(keyword in query for keyword in keywords)
    
    def detect_intent(self, query: str) -> str:
        """Keyword-only intent detection (expanded vocabulary)."""
        return self._fallback_intent_detection(query)
    
    def _fallback_intent_detection(self, query: str) -> str:
        """Fallback intent detection using keywords only"""
        query_lower = query.lower()

        # First: domain keywords take precedence
        has_price = any(keyword in query_lower for keyword in self.price_keywords)
        has_product = any(keyword in query_lower for keyword in self.product_keywords)
        has_medical = any(keyword in query_lower for keyword in self.medical_keywords)
        has_web = any(keyword in query_lower for keyword in self.web_keywords)
        has_faq = any(keyword in query_lower for keyword in self.faq_keywords) 

        # Top-level web intent
        if has_web and not has_medical:
            return "web"

        if has_price and (has_product or has_medical):
            return "price"
        if has_product:
            return "product"
        if has_price:
            return "price"
        if has_medical:
            return "medical"
        if has_faq:
            return "faq"

        # Then: greeting only if no domain keywords and it's truly short/non-question
        greeting_patterns = [
            "xin ch√†o", "ch√†o", "hello", "hi", "hey", "good morning", "good afternoon", "good evening",
            "ch√†o b·∫°n", "ch√†o bot", "ch√†o chatbot", "hello bot", "hi bot", "hey bot",
            "c·∫£m ∆°n", "thank you", "thanks", "c·∫£m ∆°n b·∫°n", "thank you bot",
            "t·∫°m bi·ªát", "goodbye", "bye", "see you", "h·∫πn g·∫∑p l·∫°i",
            "b·∫°n kh·ªèe kh√¥ng", "how are you", "b·∫°n th·∫ø n√†o", "how are you doing",
            "l√†m g√¨", "what are you doing", "b·∫°n ƒëang l√†m g√¨"
        ]
        if any(pattern in query_lower for pattern in greeting_patterns):
            is_short = len(query_lower) <= 40
            ask_markers = ["?", "cho h·ªèi", "vui l√≤ng", "mu·ªën h·ªèi", "t∆∞ v·∫•n"]
            has_question = any(m in query_lower for m in ask_markers)
            if is_short and not has_question:
                return "greeting"

        # Semantic fallback: use embeddings to choose best intent if keywords fail
        best_intent, score = self._semantic_intent(query)
        if score >= 0.45:
            return best_intent

        return "general"
    
    def validate_output(self, response: str, is_medical: bool = True) -> str:
        """Validate and enhance output"""
        if not response or not response.strip():
            return "Xin l·ªói, m√¨nh ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi ph√π h·ª£p. B·∫°n th·ª≠ h·ªèi c√°ch kh√°c gi√∫p m√¨nh nh√©."
        
        # Check for harmful content
        harmful_patterns = [
            r"t·ª±\s+ch·∫©n\s+ƒëo√°n",
            r"kh√¥ng\s+c·∫ßn\s+b√°c\s+sƒ©",
            r"t·ª±\s+ƒëi·ªÅu\s+tr·ªã",
            r"b·ªè\s+qua\s+b√°c\s+sƒ©"
        ]
        
        for pattern in harmful_patterns:
            if re.search(pattern, response.lower()):
                if "tham kh·∫£o √Ω ki·∫øn b√°c sƒ©" not in response.lower():
                    response += "\n\n‚ö†Ô∏è L∆∞u √Ω: Lu√¥n tham kh·∫£o √Ω ki·∫øn b√°c sƒ© tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh ƒëi·ªÅu tr·ªã."
        
        # Add medical disclaimer
        if is_medical and ("disclaimer" not in response.lower() and "tuy√™n b·ªë" not in response.lower()):
            # Ensure newline separation
            if not response.endswith("\n"):
                response += "\n"
            response += self.medical_disclaimer
        
        # Limit response length
        if len(response) > 2000:
            response = response[:2000].rstrip() + "...\n\n[Ph·∫£n h·ªìi ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn]"
        
        return response
    
    def is_medical_question(self, user_message: str) -> bool:
        """Check if the question is medical-related"""
        message_lower = user_message.lower()
        return any(keyword in message_lower for keyword in self.medical_keywords)
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Extract medical entities from text using LLM when available, fallback to regex."""
        # Try LLM-based extraction via LangChainIntentClassifier
        try:
            from intent_classifier import LangChainIntentClassifier
            clf = LangChainIntentClassifier()
            result = clf.classify(text)
            ent = result.get("entities") or {}
            # Normalize structure
            normalized = {
                "symptoms": ent.get("symptom") or ent.get("symptoms") or [],
                "medications": ent.get("medication") or ent.get("medications") or [],
                "conditions": ent.get("condition") or ent.get("conditions") or [],
                "product_name": ent.get("product_name") or ent.get("product") or "",
            }
            # Ensure lists
            for k in ["symptoms", "medications", "conditions"]:
                v = normalized.get(k)
                if isinstance(v, str):
                    normalized[k] = [v]
                elif not isinstance(v, list):
                    normalized[k] = []
            if not isinstance(normalized.get("product_name"), str):
                normalized["product_name"] = ""
            return normalized
        except Exception:
            pass

        # Regex fallback
        entities = {
            "symptoms": [],
            "medications": [],
            "conditions": [],
            "product_name": ""
        }
        text_lower = text.lower()
        symptom_patterns = [
            r"ƒëau\s+([\w√†√°·∫°√£·∫£ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë-]+)",
            r"s·ªët\s+([\w√†√°·∫°√£·∫£ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë-]+)",
            r"ho\s+([\w√†√°·∫°√£·∫£ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë-]+)",
        ]
        for pattern in symptom_patterns:
            entities["symptoms"].extend(re.findall(pattern, text_lower))
        return entities
